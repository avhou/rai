
@article{reviewed,
	title = {On the connection between pre-training data diversity and fine-tuning robustness},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/d1786f5246c67eefde011599d31b2006-Abstract-Conference.html},
	urldate = {2024-03-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ramanujan, Vivek and Nguyen, Thao and Oh, Sewoong and Farhadi, Ali and Schmidt, Ludwig},
	year = {2024},
	file = {Available Version (via Google Scholar):/Users/alexander/Zotero/storage/6C8WIK72/Ramanujan et al. - 2024 - On the connection between pre-training data divers.pdf:application/pdf},
}

@misc{accuracy1,
	title = {What makes {ImageNet} good for transfer learning?},
	url = {http://arxiv.org/abs/1608.08614},
	doi = {10.48550/arXiv.1608.08614},
	abstract = {The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks begs the question: what are the properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? To answer these and related questions, we pre-trained CNN features on various subsets of the ImageNet dataset and evaluated transfer performance on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical do not significantly affect transfer performance.? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A.},
	month = dec,
	year = {2016},
	note = {arXiv:1608.08614 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/QQBHTLAY/Huh et al. - 2016 - What makes ImageNet good for transfer learning.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/PXTD732X/1608.html:text/html},
}

@inproceedings{accuracy2,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Big {Transfer} ({BiT}): {General} {Visual} {Representation} {Learning}},
	isbn = {978-3-030-58558-7},
	shorttitle = {Big {Transfer} ({BiT})},
	doi = {10.1007/978-3-030-58558-7_29},
	abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes—from 1 example per class to 1M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {491--507},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/E39L2UDW/Kolesnikov et al. - 2020 - Big Transfer (BiT) General Visual Representation .pdf:application/pdf},
}

@inproceedings{accuracy3,
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.html},
	urldate = {2024-03-09},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	year = {2019},
	pages = {2661--2671},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/I8CV64IM/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf},
}

@inproceedings{variation3,
	title = {{SpotTune}: {Transfer} {Learning} {Through} {Adaptive} {Fine}-{Tuning}},
	shorttitle = {{SpotTune}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Guo_SpotTune_Transfer_Learning_Through_Adaptive_Fine-Tuning_CVPR_2019_paper.html},
	urldate = {2024-03-09},
	author = {Guo, Yunhui and Shi, Honghui and Kumar, Abhishek and Grauman, Kristen and Rosing, Tajana and Feris, Rogerio},
	year = {2019},
	pages = {4805--4814},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/VNLZZ5N6/Guo et al. - 2019 - SpotTune Transfer Learning Through Adaptive Fine-.pdf:application/pdf},
}

@inproceedings{variation2,
	title = {Do {Adversarially} {Robust} {ImageNet} {Models} {Transfer} {Better}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/24357dd085d2c4b1a88a7e0692e60294-Abstract.html},
	abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Code and models is available in the supplementary material.},
	urldate = {2024-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
	year = {2020},
	pages = {3533--3545},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/4Q522YG8/Salman et al. - 2020 - Do Adversarially Robust ImageNet Models Transfer B.pdf:application/pdf},
}

@inproceedings{variation1,
	title = {Co-{Tuning} for {Transfer} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c8067ad1937f728f51288b3eb986afaa-Abstract.html},
	urldate = {2024-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {You, Kaichao and Kou, Zhi and Long, Mingsheng and Wang, Jianmin},
	year = {2020},
	pages = {17236--17246},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/78CS4ZDR/You et al. - 2020 - Co-Tuning for Transfer Learning.pdf:application/pdf},
}

@inproceedings{fractal,
	title = {Pre-training without {Natural} {Images}},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.html},
	language = {en},
	urldate = {2024-03-09},
	author = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
	year = {2020},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/567CQ7G5/Kataoka et al. - 2020 - Pre-training without Natural Images.pdf:application/pdf},
}

@inproceedings{stable,
	title = {High-{Resolution} {Image} {Synthesis} {With} {Latent} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-03-09},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year = {2022},
	pages = {10684--10695},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/NQIGJ2FE/Rombach et al. - 2022 - High-Resolution Image Synthesis With Latent Diffus.pdf:application/pdf},
}

@inproceedings{past1,
	title = {Data {Determines} {Distributional} {Robustness} in {Contrastive} {Language} {Image} {Pre}-training ({CLIP})},
	url = {https://proceedings.mlr.press/v162/fang22a.html},
	abstract = {Contrastively trained language-image models such as CLIP, ALIGN, and BASIC have demonstrated unprecedented robustness to multiple challenging natural distribution shifts. Since these language-image models differ from previous training approaches in several ways, an important question is what causes the large robustness gains. We answer this question via a systematic experimental investigation. Concretely, we study five different possible causes for the robustness gains: (i) the training set size, (ii) the training distribution, (iii) language supervision at training time, (iv) language supervision at test time, and (v) the contrastive loss function. Our experiments show that the more diverse training distribution is the main cause for the robustness gains, with the other factors contributing little to no robustness. Beyond our experimental results, we also introduce ImageNet-Captions, a version of ImageNet with original text annotations from Flickr, to enable further controlled experiments of language-image training.},
	language = {en},
	urldate = {2024-03-09},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fang, Alex and Ilharco, Gabriel and Wortsman, Mitchell and Wan, Yuhao and Shankar, Vaishaal and Dave, Achal and Schmidt, Ludwig},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {6216--6234},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/KHXT9DXD/Fang et al. - 2022 - Data Determines Distributional Robustness in Contr.pdf:application/pdf},
}

@inproceedings{past2,
	title = {Accuracy on the {Line}: on the {Strong} {Correlation} {Between} {Out}-of-{Distribution} and {In}-{Distribution} {Generalization}},
	shorttitle = {Accuracy on the {Line}},
	url = {https://proceedings.mlr.press/v139/miller21b.html},
	abstract = {For machine learning systems to be reliable, we must understand their performance in unseen, out- of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of- distribution performance on variants of CIFAR- 10 \& ImageNet, a synthetic pose estimation task derived from YCB objects, FMoW-WILDS satellite imagery classification, and wildlife classification in iWildCam-WILDS. The correlation holds across model architectures, hyperparameters, training set size, and training duration, and is more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations.},
	language = {en},
	urldate = {2024-03-09},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Miller, John P. and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7721--7735},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/NDA284IF/Miller et al. - 2021 - Accuracy on the Line on the Strong Correlation Be.pdf:application/pdf},
}

@article{past3,
	title = {Quality {Not} {Quantity}: {On} the {Interaction} between {Dataset} {Design} and {Robustness} of {CLIP}},
	volume = {35},
	shorttitle = {Quality {Not} {Quantity}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/86a8a512b27f49519594ebe89f66d708-Abstract-Conference.html},
	language = {en},
	urldate = {2024-03-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Nguyen, Thao and Ilharco, Gabriel and Wortsman, Mitchell and Oh, Sewoong and Schmidt, Ludwig},
	month = dec,
	year = {2022},
	pages = {21455--21469},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/XUPPXGNY/Nguyen et al. - 2022 - Quality Not Quantity On the Interaction between D.pdf:application/pdf},
}

@inproceedings{add1,
	title = {How robust are pre-trained models to distribution shift?},
	url = {https://openreview.net/forum?id=zKDcZBVVEWm},
	abstract = {The vulnerability of machine learning models to spurious correlations has mostly been discussed in the context of supervised learning (SL). However, there is a lack of insight on how spurious correlations affect the performance of popular self-supervised learning (SSL) and auto-encoder based models (AE). In this work, we shed light on this by evaluating the performance of these models on both real world and synthetic distribution shift datasets. Following observations that the linear head itself can be susceptible to spurious correlations, we develop a new evaluation scheme with the linear head trained on out-of-distribution (OOD) data, to isolate the performance of the pre-trained models from a potential bias of the linear head used for evaluation. With this new methodology, we show that SSL models are consistently more robust to distribution shifts and thus better at OOD generalisation than AE and SL models.},
	language = {en},
	urldate = {2024-03-09},
	author = {Shi, Yuge and Daunhawer, Imant and Vogt, Julia E. and Torr, Philip and Sanyal, Amartya},
	month = jul,
	year = {2022},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/5T2IXE98/Shi et al. - 2022 - How robust are pre-trained models to distribution .pdf:application/pdf},
}

@misc{add2,
	title = {An {Empirical} {Study} on {Distribution} {Shift} {Robustness} {From} the {Perspective} of {Pre}-{Training} and {Data} {Augmentation}},
	url = {http://arxiv.org/abs/2205.12753},
	doi = {10.48550/arXiv.2205.12753},
	abstract = {The performance of machine learning models under distribution shift has been the focus of the community in recent years. Most of current methods have been proposed to improve the robustness to distribution shift from the algorithmic perspective, i.e., designing better training algorithms to help the generalization in shifted test distributions. This paper studies the distribution shift problem from the perspective of pre-training and data augmentation, two important factors in the practice of deep learning that have not been systematically investigated by existing work. By evaluating seven pre-trained models, including ResNets and ViT's with self-supervision and supervision mode, on five important distribution-shift datasets, from WILDS and DomainBed benchmarks, with five different learning algorithms, we provide the first comprehensive empirical study focusing on pre-training and data augmentation. With our empirical result obtained from 1,330 models, we provide the following main observations: 1) ERM combined with data augmentation can achieve state-of-the-art performance if we choose a proper pre-trained model respecting the data property; 2) specialized algorithms further improve the robustness on top of ERM when handling a specific type of distribution shift, e.g., GroupDRO for spurious correlation and CORAL for large-scale out-of-distribution data; 3) Comparing different pre-training modes, architectures and data sizes, we provide novel observations about pre-training on distribution shift, which sheds light on designing or selecting pre-training strategy for different kinds of distribution shifts. In summary, our empirical study provides a comprehensive baseline for a wide range of pre-training models fine-tuned with data augmentation, which potentially inspires research exploiting the power of pre-training and data augmentation in the future of distribution shift study.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Liu, Ziquan and Xu, Yi and Xu, Yuanhong and Qian, Qi and Li, Hao and Jin, Rong and Ji, Xiangyang and Chan, Antoni B.},
	month = may,
	year = {2022},
	note = {arXiv:2205.12753 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/ALHILFU7/Liu et al. - 2022 - An Empirical Study on Distribution Shift Robustnes.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/JTLW2FG9/2205.html:text/html},
}

@inproceedings{add4,
	title = {Using {Pre}-{Training} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	url = {https://proceedings.mlr.press/v97/hendrycks19a.html},
	abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
	language = {en},
	urldate = {2024-03-09},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2712--2721},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/95SUUIJT/Hendrycks et al. - 2019 - Using Pre-Training Can Improve Model Robustness an.pdf:application/pdf;Supplementary PDF:/Users/alexander/Zotero/storage/PGHMFGV7/Hendrycks et al. - 2019 - Using Pre-Training Can Improve Model Robustness an.pdf:application/pdf},
}
